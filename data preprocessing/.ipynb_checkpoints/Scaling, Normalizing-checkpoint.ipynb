{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Feature scaling in data science involves normalizing or standardizing numerical features to bring them to a similar scale. This ensures that no particular feature dominates the others, preventing biases in data science models. \n",
    "\n",
    "\n",
    "Additional reasons for transformation:\n",
    "\n",
    "1. To more closely approximate a theoretical distribution that has nice statistical properties. \n",
    "2. To spread out data more evenly.\n",
    "3. To make data distribution more symmetric\n",
    "4. to make relationships between variables more linear. \n",
    "5. TO make data more constant in variance (homoscedasticity). \n",
    "\n",
    "#### There are 3 most used ways to scale features. \n",
    "1. __Min Max Scaling__: \n",
    "Will scale the input to have minimum of 0 and maximum of 1. That is, it scales the data in the range of [0, 1] This is useful when the parameters have to be on same positive scale. But in this case, the outliers are lost. \n",
    "$$X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "2. __Standardization__:\n",
    "Will scale the input to have mean of 0 and variance of 1. \n",
    "$$X_{stand} = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "3. __Normalizing__: \n",
    "Will scale the input to make the norm of 1. For instance, for 3D data the 3 independent variables will lie on a unit Sphere. \n",
    "\n",
    "4. __Log Transformation__:\n",
    "Taking the log of data after any of above transformation. \n",
    "\n",
    "Scaling inputs to unit norms is a common operation for text classification or clustering for instance.\n",
    "\n",
    "For most applications, Standardization is recommended. Min Max Scaling is recommended for Neural Networks. Normalizing is recommended when Clustering eg. KMeans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "5   France  35.0  58000.0       Yes\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
    "\n",
    "df = pd.read_csv('Data.csv').dropna()\n",
    "print(df)\n",
    "X = df[[\"Age\", \"Salary\"]].values.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**: apply the [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), [`Normalizer()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html), and [`MinMaxScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) methods to the data `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization\n",
      "[[4.4e+01 7.2e+04]\n",
      " [2.7e+01 4.8e+04]\n",
      " [3.0e+01 5.4e+04]\n",
      " [3.8e+01 6.1e+04]\n",
      " [3.5e+01 5.8e+04]\n",
      " [4.8e+01 7.9e+04]\n",
      " [5.0e+01 8.3e+04]\n",
      " [3.7e+01 6.7e+04]]\n",
      "Normalizing\n",
      "[[4.4e+01 7.2e+04]\n",
      " [2.7e+01 4.8e+04]\n",
      " [3.0e+01 5.4e+04]\n",
      " [3.8e+01 6.1e+04]\n",
      " [3.5e+01 5.8e+04]\n",
      " [4.8e+01 7.9e+04]\n",
      " [5.0e+01 8.3e+04]\n",
      " [3.7e+01 6.7e+04]]\n",
      "MinMax Scaling\n",
      "[[4.4e+01 7.2e+04]\n",
      " [2.7e+01 4.8e+04]\n",
      " [3.0e+01 5.4e+04]\n",
      " [3.8e+01 6.1e+04]\n",
      " [3.5e+01 5.8e+04]\n",
      " [4.8e+01 7.9e+04]\n",
      " [5.0e+01 8.3e+04]\n",
      " [3.7e+01 6.7e+04]]\n"
     ]
    }
   ],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "normalizer = Normalizer()\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "print(\"Standardization\")\n",
    "X_standardized = standard_scaler.fit_transform(X)\n",
    "print(X)\n",
    "\n",
    "print(\"Normalizing\")\n",
    "X_normalized = normalizer.fit_transform(X)\n",
    "print(X)\n",
    "\n",
    "print(\"MinMax Scaling\")\n",
    "X_minmax = min_max_scaler.fit_transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
